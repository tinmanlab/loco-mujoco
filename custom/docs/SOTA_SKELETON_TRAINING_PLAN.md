# SkeletonTorque SOTA Training Plan
## Biomechanical + Robust + Multi-motion Humanoid Control

**ëª©í‘œ**: `models/skeleton` XML ê¸°ë°˜ìœ¼ë¡œ SOTA ìˆ˜ì¤€ì˜ biomechanical humanoid locomotion ë‹¬ì„±

**ìš”êµ¬ì‚¬í•­**:
- âœ… ë‹¤ì–‘í•œ ëª¨ì…˜ (walk, run, jump, recovery, etc.)
- âœ… ë„˜ì–´ì§€ì§€ ì•ŠëŠ” ì•ˆì •ì„±
- âœ… ì™¸ë€ì— ê°•í•œ robustness
- âœ… Biomechanical realism
- âœ… MuJoCo/MJX ê¸°ë°˜

---

## ğŸ” SOTA ì—°êµ¬ ë¶„ì„ ê²°ê³¼

### ìµœì‹  ì ‘ê·¼ë²• (2024-2025)

| í”„ë¡œì íŠ¸ | í•µì‹¬ ê¸°ë²• | Biomechanical | Robustness | MuJoCo |
|---------|----------|---------------|-----------|--------|
| **ALMI** | Upper/Lower adversarial | â­â­â­ | â­â­â­â­ | âœ… |
| **ResMimic** | Two-stage residual | â­â­â­â­ | â­â­â­â­ | âœ… (eval) |
| **HumanoidBench** | Hierarchical RL | â­â­â­ | â­â­â­â­ | âœ… |
| **MuJoCo Playground** | PPO + DR | â­â­ | â­â­â­â­â­ | âœ… |
| **LocoMuJoCo** | AMP/GAIL | â­â­â­â­ | â­â­ | âœ… |

### ê³µí†µ ì„±ê³µ ìš”ì†Œ

1. **Large-scale motion data** (15,000+ clips)
2. **Multi-stage training** (base skills â†’ task-specific)
3. **Hierarchical control** (low-level + high-level)
4. **Perturbation training** (force + domain randomization)
5. **Recovery behaviors** (explicit fall recovery)

---

## ğŸ¯ SkeletonTorque ìµœì  ì ‘ê·¼ë²•

### ì™œ ë‹¨ìˆœ PPO + DRë¡œëŠ” ë¶€ì¡±í•œê°€?

âŒ **MuJoCo Playground í•œê³„**:
- ë‹¨ìˆœ velocity trackingë§Œ í•™ìŠµ
- Motion quality/naturalness ë¶€ì¡±
- Biomechanical constraints ë¬´ì‹œ
- Recovery behaviors ì—†ìŒ

âŒ **í˜„ì¬ AMP í•™ìŠµ í•œê³„**:
- ë‹¨ì¼ motionë§Œ í•™ìŠµ (run)
- Perturbation ì—†ìŒ
- Recovery ì—†ìŒ
- Task complexity ë¶€ì¡±

âœ… **í•„ìš”í•œ ê²ƒ**:
- **Natural motion patterns** (biomechanical realism)
- **Diverse skills** (walk, run, jump, recover)
- **Robust to perturbations** (force, mass, friction)
- **Long-term stability** (never fall)
- **Adaptive recovery** (ìë™ ê· í˜• íšŒë³µ)

---

## ğŸš€ ì œì•ˆ: Multi-Stage Hierarchical Training

### Stage 1: Large-Scale Motion Imitation (GMT)
**ëª©í‘œ**: Natural, biomechanically realistic movements

```yaml
# conf_stage1_gmt.yaml

experiment:
  name: "SkeletonTorque_GMT_Stage1"

  task_factory:
    name: ImitationFactory
    params:
      # LocoMuJoCoì˜ 22,000+ mocap ìµœëŒ€ í™œìš©
      default_dataset_conf:
          task: [walk, run, pace, trot, jump, squat, dance]
      lafan1_dataset_conf:
          task: [walk1_subject1, walk2_subject2, run1_subject1,
                 dance1_subject1, dance2_subject4]
      # AMASS datasetë„ ì¶”ê°€ ê°€ëŠ¥ (manual download)

  env_params:
    env_name: MjxSkeletonTorque
    headless: True
    horizon: 1000
    # Motion tracking reward (no task-specific goals yet)
    reward_type: MotionTrackingReward  # Pure imitation

  algorithm: AMP  # or GAIL

  # Large-scale training
  total_timesteps: 500e6  # 500M! (ì•½ 1ì£¼ì¼)
  num_envs: 4096

  # Network capacity ì¦ê°€ (diverse motions)
  hidden_layers: [1024, 512, 256, 256]

  # Imitation ê°•ë„
  proportion_env_reward: 0.0  # Pure motion imitation
  disc_lr: 3e-5
```

**ê¸°ëŒ€ íš¨ê³¼**:
- âœ… 15,000+ motion clips í•™ìŠµ
- âœ… Natural, biomechanically realistic movements
- âœ… Diverse skill repertoire
- âœ… "Style" embedding (ASEì²˜ëŸ¼)

**í•™ìŠµ ì‹œê°„**: ~7-10ì¼ (RTX 3070, 4096 envs, 500M steps)

---

### Stage 2: Robustness Training
**ëª©í‘œ**: Perturbation resistance + Recovery behaviors

```yaml
# conf_stage2_robust.yaml

experiment:
  name: "SkeletonTorque_Robust_Stage2"

  # Stage 1 checkpoint ë¡œë“œ
  pretrained_policy: "stage1_gmt/outputs/.../AMPJax_saved.pkl"

  task_factory:
    name: ImitationFactory
    params:
      default_dataset_conf:
          task: [walk, run, getup, rollover]  # Recovery motions ì¶”ê°€!

      # Perturbation wrapper
      wrappers:
        - name: PerturbationWrapper
          params:
            force_range: 150.0  # ë” ê°•í•œ ì™¸ë ¥
            force_prob: 0.2     # 20% í™•ë¥ 
            force_duration: 15  # ë” ê¸´ ì§€ì†
            bodies: [pelvis, torso, left_thigh, right_thigh,
                     left_shoulder, right_shoulder]

        - name: DomainRandomizationWrapper
          params:
            mass_range: 0.3     # Â±30%
            friction_range: 0.4 # Â±40%
            damping_range: 0.3  # Â±30%
            actuator_range: 0.15 # Â±15%

        - name: ObservationNoiseWrapper
          params:
            position_noise: 0.02
            velocity_noise: 0.15
            imu_noise: 0.08

  env_params:
    env_name: MjxSkeletonTorque
    headless: True
    horizon: 2000  # ë” ê¸´ episode (recovery í…ŒìŠ¤íŠ¸)

    # Task reward ì¶”ê°€
    reward_type: CompositeReward
    reward_components:
      - type: MotionTrackingReward
        weight: 0.5  # Motion quality ìœ ì§€
      - type: StabilityReward
        weight: 0.3  # ë„˜ì–´ì§€ì§€ ì•Šê¸°
      - type: RecoveryReward
        weight: 0.2  # ê· í˜• íšŒë³µ

  # Fine-tuning ì„¤ì •
  total_timesteps: 300e6  # 300M
  lr: 2e-5  # Lower lr (fine-tuning)

  # Curriculum learning
  curriculum:
    enabled: true
    initial_force: 20.0
    final_force: 150.0
    steps: 100e6
```

**ê¸°ëŒ€ íš¨ê³¼**:
- âœ… 150N ì™¸ë ¥ ëŒ€ì‘
- âœ… ìë™ ê· í˜• íšŒë³µ (recovery motions)
- âœ… ë‹¤ì–‘í•œ ì§€í˜•/ì¡°ê±´ ëŒ€ì‘
- âœ… Long-term stability (2000+ steps)

**í•™ìŠµ ì‹œê°„**: ~5-7ì¼

---

### Stage 3: Hierarchical Task Learning
**ëª©í‘œ**: High-level planning + Low-level execution

```yaml
# conf_stage3_hierarchical.yaml

experiment:
  name: "SkeletonTorque_Hierarchical_Stage3"

  # Two-level architecture
  architecture: Hierarchical

  # Low-level policy (from Stage 2)
  low_level:
    pretrained: "stage2_robust/outputs/.../AMPJax_saved.pkl"
    frozen: false  # Allow fine-tuning
    control_frequency: 50  # Hz

  # High-level policy (ìƒˆë¡œ í•™ìŠµ)
  high_level:
    network: [256, 128]
    control_frequency: 5  # Hz (10x slower)

    # Latent skill selection
    skill_dim: 32  # Skill embedding dimension

    # High-level observations
    obs_space:
      - target_velocity  # [vx, vy, vyaw]
      - terrain_height_map
      - external_forces  # Privileged info
      - center_of_mass

    # High-level actions
    action_space:
      - skill_embedding  # [32-dim]
      - gait_phase  # [0-1]
      - step_frequency  # [0.5-2.0 Hz]

  # Task-specific training
  task_factory:
    name: TaskFactory
    params:
      tasks:
        - VelocityTracking
        - TerrainNavigation
        - ObstacleAvoidance
        - PushRecovery

  # Asymmetric Actor-Critic
  asymmetric:
    enabled: true
    critic_obs_extra:
      - ground_truth_friction
      - mass_distribution
      - future_perturbations  # 5 steps ahead

  total_timesteps: 200e6
```

**ê¸°ëŒ€ íš¨ê³¼**:
- âœ… Adaptive skill selection
- âœ… Long-horizon planning
- âœ… Complex task execution
- âœ… Generalization to new tasks

**í•™ìŠµ ì‹œê°„**: ~4-5ì¼

---

## ğŸ› ï¸ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. Custom Wrappers

```python
# custom/wrappers/perturbation_wrapper_mjx.py

import jax
import jax.numpy as jnp
from functools import partial
from typing import Dict, Any


class PerturbationWrapperMJX:
    """
    MJX í™˜ê²½ì— ëœë¤ ì™¸ë ¥ ì ìš©
    - Curriculum learning ì§€ì›
    - Body-specific force application
    - Recovery behavior triggering
    """

    def __init__(self, env, config: Dict[str, Any]):
        self.env = env
        self.config = config

        # Curriculum settings
        self.curriculum_enabled = config.get('curriculum', {}).get('enabled', False)
        self.initial_force = config.get('curriculum', {}).get('initial_force', 20.0)
        self.final_force = config.get('force_range', 100.0)
        self.curriculum_steps = config.get('curriculum', {}).get('steps', 100e6)

        # Perturbation settings
        self.force_prob = config.get('force_prob', 0.1)
        self.force_duration = config.get('force_duration', 10)
        self.body_names = config.get('bodies', ['pelvis', 'torso'])

        # Get body IDs from names
        self.body_ids = [self.env.model.body(name).id
                        for name in self.body_names
                        if name in self.env.model.names]

    @partial(jax.jit, static_argnums=(0,))
    def get_current_force_range(self, timestep):
        """Curriculum learning: gradually increase force"""
        if not self.curriculum_enabled:
            return self.final_force

        progress = jnp.minimum(timestep / self.curriculum_steps, 1.0)
        current_force = (self.initial_force +
                        (self.final_force - self.initial_force) * progress)
        return current_force

    @partial(jax.jit, static_argnums=(0,))
    def apply_perturbation(self, env_state, timestep, rng):
        """Apply random external force to random body"""
        rng, force_rng, body_rng, dir_rng = jax.random.split(rng, 4)

        # Check if we should apply force
        should_apply = jax.random.bernoulli(force_rng, self.force_prob)

        # Get current force range (curriculum)
        force_range = self.get_current_force_range(timestep)

        # Random force magnitude and direction
        force_mag = jax.random.uniform(force_rng, minval=0.0, maxval=force_range)
        force_dir = jax.random.normal(dir_rng, shape=(3,))
        force_dir = force_dir / jnp.linalg.norm(force_dir)  # Normalize
        force = force_mag * force_dir

        # Random body selection
        body_idx = jax.random.randint(body_rng, shape=(),
                                     minval=0, maxval=len(self.body_ids))
        body_id = self.body_ids[body_idx]

        # Apply force to env_state
        xfrc_applied = env_state.data.xfrc_applied.at[body_id, :3].set(
            jnp.where(should_apply, force, jnp.zeros(3))
        )

        env_state = env_state.replace(
            data=env_state.data.replace(xfrc_applied=xfrc_applied)
        )

        return env_state, rng
```

### 2. Recovery Reward

```python
# custom/rewards/recovery_reward.py

import jax.numpy as jnp


def recovery_reward(obs, prev_obs, data, config):
    """
    Reward for recovering from near-fall states
    """
    # Get pelvis height and tilt
    pelvis_height = data.qpos[2]  # Assuming 2 is z-coordinate
    pelvis_tilt = jnp.abs(data.qpos[3:6])  # Roll, pitch, yaw

    # Get COM velocity
    com_vel = jnp.linalg.norm(data.qvel[:3])

    # Near-fall detection
    min_height = config.get('min_safe_height', 0.8)
    max_tilt = config.get('max_safe_tilt', 0.3)

    is_near_fall = jnp.logical_or(
        pelvis_height < min_height,
        jnp.max(pelvis_tilt) > max_tilt
    )

    # Previous state
    prev_pelvis_height = prev_obs[2]

    # Recovery detected if:
    # 1. Was near-fall
    # 2. Now recovering (height increasing, tilt decreasing)
    height_improvement = pelvis_height - prev_pelvis_height
    is_recovering = jnp.logical_and(
        is_near_fall,
        height_improvement > 0.0
    )

    # Reward recovery effort
    recovery_reward = jnp.where(
        is_recovering,
        height_improvement * 10.0,  # Encourage height recovery
        0.0
    )

    # Bonus for successful recovery
    recovery_success = jnp.logical_and(
        is_recovering,
        pelvis_height > min_height
    )
    recovery_bonus = jnp.where(recovery_success, 5.0, 0.0)

    return recovery_reward + recovery_bonus
```

### 3. Composite Reward

```python
# custom/rewards/composite_reward.py

from loco_mujoco.core.reward import RewardBase


class CompositeReward(RewardBase):
    """
    Multi-objective reward combining:
    - Motion tracking (biomechanical realism)
    - Stability (don't fall)
    - Recovery (balance recovery)
    - Task progress (velocity tracking, etc.)
    """

    def __init__(self, env, config):
        super().__init__(env)

        # Component weights
        self.weights = {
            'motion_tracking': config.get('motion_weight', 0.4),
            'stability': config.get('stability_weight', 0.3),
            'recovery': config.get('recovery_weight', 0.2),
            'task': config.get('task_weight', 0.1)
        }

        # Individual reward components
        from loco_mujoco.core.reward.imitation import MotionTrackingReward
        from custom.rewards.recovery_reward import recovery_reward

        self.motion_reward = MotionTrackingReward(env, config)
        self.recovery_fn = recovery_reward

    def __call__(self, obs, action, next_obs, absorbing):
        """Compute composite reward"""

        # 1. Motion tracking (biomechanical realism)
        motion_rew = self.motion_reward(obs, action, next_obs, absorbing)

        # 2. Stability reward
        pelvis_height = next_obs[2]
        min_height = 0.8
        stability_rew = jnp.where(
            pelvis_height > min_height,
            1.0,
            -10.0  # Heavy penalty for falling
        )

        # 3. Recovery reward
        recovery_rew = self.recovery_fn(
            next_obs, obs, self.env._data, self.config
        )

        # 4. Task reward (velocity tracking, etc.)
        target_vel = self.env.goal  # Assuming velocity goal
        actual_vel = next_obs[10:13]  # Assuming COM velocity
        vel_error = jnp.linalg.norm(target_vel - actual_vel)
        task_rew = jnp.exp(-vel_error)

        # Weighted sum
        total_reward = (
            self.weights['motion_tracking'] * motion_rew +
            self.weights['stability'] * stability_rew +
            self.weights['recovery'] * recovery_rew +
            self.weights['task'] * task_rew
        )

        return total_reward
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### Stage 1 ì™„ë£Œ í›„:
- âœ… 15,000+ motion clips ì¬í˜„ ê°€ëŠ¥
- âœ… Natural, biomechanically realistic movements
- âœ… Diverse skill repertoire (walk, run, jump, dance, etc.)
- âš ï¸ Still fragile to perturbations

### Stage 2 ì™„ë£Œ í›„:
- âœ… 150N ì™¸ë ¥ ëŒ€ì‘ (í˜„ì¬ ëŒ€ë¹„ **10-15ë°° í–¥ìƒ**)
- âœ… ìë™ ê· í˜• íšŒë³µ (recovery rate 80%+)
- âœ… ë‹¤ì–‘í•œ ì§€í˜•/ì¡°ê±´ robust
- âœ… Episode length 5000+ steps (never fall)

### Stage 3 ì™„ë£Œ í›„:
- âœ… Complex task execution (obstacle navigation, etc.)
- âœ… Adaptive skill selection
- âœ… Long-horizon planning
- âœ… **SOTA ìˆ˜ì¤€ ë‹¬ì„±**

---

## ğŸ¯ Timeline

```
Week 1-2: Stage 1 GMT ì¤€ë¹„ ë° ì‹œì‘
  - Wrapper êµ¬í˜„
  - Large-scale dataset ì¤€ë¹„ (22,000+ mocap)
  - Training launch (500M steps, ~10ì¼)

Week 3: Stage 1 í•™ìŠµ ì¤‘ + Stage 2 ì¤€ë¹„
  - Perturbation wrapper êµ¬í˜„
  - Recovery reward êµ¬í˜„
  - Composite reward êµ¬í˜„

Week 4-5: Stage 2 Robustness í•™ìŠµ
  - Stage 1 checkpoint ë¡œë“œ
  - Perturbation training (300M steps, ~7ì¼)

Week 6: Stage 2 í‰ê°€ + Stage 3 ì¤€ë¹„
  - Robustness í…ŒìŠ¤íŠ¸ (force application)
  - Hierarchical architecture ì„¤ê³„

Week 7-8: Stage 3 Hierarchical í•™ìŠµ
  - Two-level policy training (200M steps, ~5ì¼)

Week 9: ìµœì¢… í‰ê°€ ë° íŠœë‹
  - Sim-to-real ì¤€ë¹„
  - Biomechanical validation
  - Performance benchmarking

Total: ~9ì£¼ (2ê°œì›”)
```

---

## ğŸ’¡ ì™œ ì´ ì ‘ê·¼ì´ ìµœì„ ì¸ê°€?

### 1. LocoMuJoCoì˜ ê°•ì  ìµœëŒ€ í™œìš©
- âœ… 22,000+ mocap datasets
- âœ… Biomechanically realistic skeletons
- âœ… MJX GPU parallelization
- âœ… Proven imitation learning algorithms (AMP, GAIL)

### 2. SOTA ì—°êµ¬ ë°©ë²•ë¡  í†µí•©
- âœ… ResMimicì˜ two-stage approach
- âœ… ALMIì˜ adversarial learning concept
- âœ… HumanoidBenchì˜ hierarchical control
- âœ… MuJoCo Playgroundì˜ perturbation training

### 3. Biomechanical Realism
- âœ… Large-scale motion imitation (Stage 1)
- âœ… Natural movement patterns
- âœ… Contact dynamics
- âœ… Energy-efficient gaits

### 4. SOTA Robustness
- âœ… Perturbation training (150N forces)
- âœ… Domain randomization (mass, friction, etc.)
- âœ… Recovery behaviors (explicit learning)
- âœ… Curriculum learning (gradual difficulty)

### 5. ì‹¤í–‰ ê°€ëŠ¥ì„±
- âœ… ëª¨ë“  ìš”ì†Œë¥¼ LocoMuJoCoì—ì„œ êµ¬í˜„ ê°€ëŠ¥
- âœ… ê¸°ì¡´ ì¸í”„ë¼ í™œìš© (MJX, JAX, 4096 envs)
- âœ… ë‹¨ê³„ë³„ ê²€ì¦ ê°€ëŠ¥
- âœ… 2ê°œì›” ë‚´ ì™„ë£Œ ê°€ëŠ¥

---

## ğŸ”— ì°¸ê³  ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
1. **ALMI** (2024): https://arxiv.org/abs/2504.14305
2. **ResMimic** (2024): https://arxiv.org/abs/2510.05070
3. **HumanoidBench** (2024): https://arxiv.org/abs/2403.10506
4. **LocoMuJoCo** (2023): https://arxiv.org/abs/2311.02496

### ì½”ë“œ ë² ì´ìŠ¤
- **ALMI**: https://github.com/TeleHuman/ALMI-Open
- **LocoMuJoCo**: https://github.com/robfiras/loco-mujoco
- **MuJoCo Playground**: https://github.com/google-deepmind/mujoco_playground

### Datasets
- **AMASS**: https://amass.is.tue.mpg.de/
- **LAFAN1**: https://github.com/ubisoft/ubisoft-laforge-animation-dataset
- **LocoMuJoCo Default**: Auto-download

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

### Option A: ì „ì²´ Pipeline êµ¬í˜„ (ì¶”ì²œ!)
```bash
# 1. Wrapper êµ¬í˜„ (1-2ì¼)
custom/wrappers/perturbation_wrapper_mjx.py
custom/wrappers/domain_randomization.py
custom/rewards/recovery_reward.py
custom/rewards/composite_reward.py

# 2. Stage 1 ì‹œì‘ (10ì¼)
python custom/training/stage1_gmt/train.py

# 3. Stage 2 (7ì¼)
# 4. Stage 3 (5ì¼)
# 5. í‰ê°€
```

### Option B: ë‹¨ê³„ë³„ ê²€ì¦
```bash
# ë¨¼ì € Stage 1ë§Œ êµ¬í˜„ ë° ê²€ì¦
# ì„±ê³µ í›„ Stage 2, 3 ìˆœì°¨ ì§„í–‰
```

### Option C: Simplified Version
```bash
# Hierarchical ì—†ì´ Stage 1 + 2ë§Œ
# ì¶©ë¶„í•œ robustness ë‹¬ì„± ê°€ëŠ¥
```

---

## ğŸ¯ ê²°ë¡ 

**SkeletonTorque SOTA í•™ìŠµì„ ìœ„í•œ ìµœì„ ì˜ ë°©ë²•:**

âœ… **Multi-Stage Training**:
1. Large-scale motion imitation (GMT)
2. Perturbation + Recovery training
3. Hierarchical task learning

âœ… **LocoMuJoCo ìµœëŒ€ í™œìš©**:
- 22,000+ mocap datasets
- MJX GPU parallelization
- Biomechanical skeletons

âœ… **SOTA ë°©ë²•ë¡  í†µí•©**:
- ResMimic two-stage
- ALMI adversarial
- HumanoidBench hierarchical
- Playground perturbation

âœ… **2ê°œì›” ë‚´ ì™„ë£Œ ê°€ëŠ¥**
âœ… **Biomechanical + Robust + Multi-motion**

**ë‹¤ìŒ: Stage 1 GMT wrapper êµ¬í˜„ë¶€í„° ì‹œì‘!**
