# í˜„ì¬ ëª¨ë¸ ë¶„ì„ ë° ê°œì„  ë°©ì•ˆ

## ğŸ“Š í˜„ì¬ í•™ìŠµëœ ëª¨ë¸ ë¶„ì„

### UnitreeH1 AMP Training

**í•™ìŠµ ê¸°ë²•: AMP (Adversarial Motion Priors)**
- **ì•Œê³ ë¦¬ì¦˜**: Imitation Learning + PPO
- **í•µì‹¬ ì•„ì´ë””ì–´**: Discriminatorë¥¼ ì‚¬ìš©í•´ mocap dataì˜ "style"ì„ í•™ìŠµ
- **Reward**: Task reward (50%) + Style reward from discriminator (50%)
- **Dataset**: "run" mocap (ë‹¬ë¦¬ê¸° ë™ì‘)
- **Timesteps**: 75M
- **í™˜ê²½ ìˆ˜**: 4096 parallel environments (RTX 3070 ìµœì í™”)

**í•™ìŠµ ì„¤ì • (conf.yaml ë¶„ì„):**
```yaml
Algorithm: AMP
Dataset: run (default LocoMuJoCo dataset)
Total timesteps: 75M
Learning rate: 6e-5
Discriminator lr: 5e-5
Network: [512, 256] hidden layers
```

**ê²°ê³¼:**
- âœ… Mean Episode Return: 154.12
- âœ… Mean Episode Length: 965.83
- âœ… ë‹¬ë¦¬ê¸° ë™ì‘ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜í–‰
- âŒ **ì™¸ë ¥ì— ë§¤ìš° ì·¨ì•½** - ì¡°ê¸ˆë§Œ ë°€ì–´ë„ ë„˜ì–´ì§
- âŒ í•œ ê°€ì§€ ë™ì‘ë§Œ í•™ìŠµ (run)

---

## ğŸ” ì™œ ì™¸ë ¥ì— ì•½í•œê°€?

### 1. **Perturbation Training ì—†ìŒ**
í˜„ì¬ í•™ìŠµ ì„¤ì •ì— **ì™¸ë ¥ ì ìš©ì´ ì „í˜€ í¬í•¨ë˜ì§€ ì•ŠìŒ**:
- í•™ìŠµ ì¤‘ ë¬´ì‘ìœ„ ì™¸ë ¥ ì ìš© âŒ
- Domain randomization âŒ
- Adversarial perturbation âŒ

â†’ ë¡œë´‡ì´ "ì™„ë²½í•œ í™˜ê²½"ì—ì„œë§Œ ê±·ëŠ” ë²•ì„ ë°°ì›€

### 2. **AMPì˜ í•œê³„**
AMPëŠ” **mocap trajectoryë¥¼ ë”°ë¼ê°€ëŠ” ê²ƒ**ì´ ëª©í‘œ:
- ì™¸ë ¥ ëŒ€ì‘ì´ rewardì— í¬í•¨ë˜ì§€ ì•ŠìŒ
- "ì˜ˆì˜ê²Œ ê±·ê¸°"ë§Œ í•™ìŠµ, "ê· í˜• íšŒë³µ"ì€ í•™ìŠµí•˜ì§€ ì•ŠìŒ

### 3. **ë‹¨ì¼ ë™ì‘ë§Œ í•™ìŠµ**
- "run" mocapë§Œ ì‚¬ìš©
- ë‹¤ì–‘í•œ ìƒí™© ëŒ€ì‘ ëŠ¥ë ¥ ë¶€ì¡±
- Recovery motion ì—†ìŒ

---

## ğŸ¯ ê°œì„  ë°©ì•ˆ

### Level 1: ê¸°ë³¸ Robustness í–¥ìƒ (êµìœ¡/ì—°êµ¬ìš© â†’ ì‹¤ìš© ì—°êµ¬ìš©)

#### 1.1 Perturbation Training ì¶”ê°€
**Custom wrapper êµ¬í˜„ í•„ìš”:**

```python
class PerturbationWrapper:
    def __init__(self, env, force_range=50.0, prob=0.1):
        self.env = env
        self.force_range = force_range  # ìµœëŒ€ í˜ (N)
        self.prob = prob  # ë§¤ step í™•ë¥ 

    def step(self, action):
        obs, reward, done, info = self.env.step(action)

        # ëœë¤ìœ¼ë¡œ ì™¸ë ¥ ì ìš©
        if np.random.rand() < self.prob:
            body_id = np.random.randint(self.env.model.nbody)
            force = np.random.uniform(-self.force_range,
                                     self.force_range, 3)
            self.env.data.xfrc_applied[body_id, :3] = force

        return obs, reward, done, info
```

**íš¨ê³¼:**
- âœ… ì™¸ë ¥ì— ëŒ€í•œ ëŒ€ì‘ ëŠ¥ë ¥ í•™ìŠµ
- âœ… ê· í˜• íšŒë³µ ëŠ¥ë ¥ í–¥ìƒ
- âš ï¸ í•™ìŠµ ì‹œê°„ ì¦ê°€ (100M+ timesteps í•„ìš”)

#### 1.2 ë‹¤ì–‘í•œ Motion í•™ìŠµ
```yaml
default_dataset_conf:
    task: [walk, run, pace, trot]  # ì—¬ëŸ¬ ë™ì‘ ë™ì‹œ í•™ìŠµ
```

**íš¨ê³¼:**
- âœ… ë‹¤ì–‘í•œ ì†ë„/ìŠ¤íƒ€ì¼ë¡œ ì´ë™ ê°€ëŠ¥
- âœ… ìƒí™©ì— ë§ëŠ” ë™ì‘ ì„ íƒ ëŠ¥ë ¥
- âœ… ì „ì´ í•™ìŠµ(transition) ëŠ¥ë ¥ í–¥ìƒ

#### 1.3 Domain Randomization
```yaml
env_params:
  randomize_friction: true      # ë§ˆì°°ë ¥ ëœë¤í™”
  randomize_mass: true          # ì§ˆëŸ‰ ëœë¤í™” (Â±20%)
  randomize_actuator: true      # ì•¡ì¶”ì—ì´í„° ë…¸ì´ì¦ˆ
```

**íš¨ê³¼:**
- âœ… Sim-to-real gap ê°ì†Œ
- âœ… ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ robust
- âœ… ì‹¤ì œ ë¡œë´‡ ì ìš© ê°€ëŠ¥ì„± ì¦ê°€

---

### Level 2: State-of-the-Art Robustness (ì—°êµ¬ ìµœì „ì„ )

#### 2.1 **ASE (Adversarial Skill Embeddings)**
- **ë…¼ë¬¸**: "ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters" (SIGGRAPH 2022)
- **íŠ¹ì§•**:
  - ë‹¤ì–‘í•œ mocap ë™ì‘ í•™ìŠµ (1000+ clips)
  - Latent skill embedding space
  - High-level task + Low-level skill ë¶„ë¦¬
- **ì¥ì **:
  - âœ… ë§¤ìš° ë‹¤ì–‘í•œ ë™ì‘ ë ˆí¼í† ë¦¬
  - âœ… ìƒˆë¡œìš´ taskì— ë¹ ë¥´ê²Œ ì ì‘
  - âœ… Robustí•œ recovery behaviors

#### 2.2 **PHC (Perpetual Humanoid Control)**
- **ë…¼ë¬¸**: "Perpetual Humanoid Control for Real-time Simulated Avatars" (ICCV 2023)
- **íŠ¹ì§•**:
  - Self-supervised learning
  - Real-time performance
  - Long-term stability
- **ì¥ì **:
  - âœ… ë¬´í•œíˆ ì•ˆì •ì ì¸ ì œì–´
  - âœ… ì‹¤ì‹œê°„ interactive control
  - âœ… ì™¸ë ¥ ëŒ€ì‘ ëŠ¥ë ¥ íƒì›”

#### 2.3 **CALM (Composable Adversarial Learning for Motion)**
- **íŠ¹ì§•**:
  - Compositional motion primitives
  - Hierarchical policy structure
  - Adaptive recovery behaviors
- **ì¥ì **:
  - âœ… Motion primitive ì¡°í•© ê°€ëŠ¥
  - âœ… ìë™ recovery í•™ìŠµ
  - âœ… ë§¤ìš° robust

---

### Level 3: ìƒìš©/ì‹¤ì œ ë¡œë´‡ ìˆ˜ì¤€

#### 3.1 **Hierarchical Control**
```
High-level Policy (Task Planning)
    â†“
Mid-level Policy (Motion Selection)
    â†“
Low-level Policy (Joint Control)
```

**êµ¬í˜„ ë°©ë²•:**
- Teacher-Student training
- Curriculum learning
- Meta-learning for fast adaptation

**íš¨ê³¼:**
- âœ… ë³µì¡í•œ task ìˆ˜í–‰ ê°€ëŠ¥
- âœ… Long-horizon planning
- âœ… Human-like behavior

#### 3.2 **Model Predictive Control (MPC) Hybrid**
```
Learning-based Policy + MPC
    â†“
Whole-body trajectory optimization
    â†“
Safety constraints enforcement
```

**íš¨ê³¼:**
- âœ… ì•ˆì „ì„± ë³´ì¥ (safety constraints)
- âœ… ë¬¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•œ ë™ì‘
- âœ… ìµœì í™”ëœ ì—ë„ˆì§€ íš¨ìœ¨

#### 3.3 **Sim-to-Real Transfer**
- **Domain Adaptation**:
  - System Identification
  - Residual Policy Learning
  - Privileged Information Training
- **Real Robot Testing**:
  - Safety controller overlay
  - Gradual deployment
  - Real-world data fine-tuning

---

## ğŸ“š í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ê¸°ë²•ì˜ ìœ„ì¹˜

### í•™ìŠµ ê¸°ë²• ë°œì „ íƒ€ì„ë¼ì¸:

```
2018: DeepMimic (Original motion imitation)
       â†“
2021: AMP (Adversarial Motion Priors) â† í˜„ì¬ ì‚¬ìš© ì¤‘!
       â†“
2022: ASE (Large-scale skill embeddings)
       â†“
2023: PHC (Perpetual control)
       â†“
2024: Diffusion policies, Foundation models
```

**í˜„ì¬ ìˆ˜ì¤€ í‰ê°€:**
- ğŸ“ **êµìœ¡/ì—°êµ¬ìš©**: âœ… ì í•©
- ğŸ”¬ **ê³ ê¸‰ ì—°êµ¬ìš©**: âš ï¸ ë¶€ë¶„ ì í•© (robustness ë¶€ì¡±)
- ğŸ­ **ìƒìš©/ì‹¤ì œ ë¡œë´‡**: âŒ ë¶€ì í•© (ì•ˆì „ì„±, ê°•ê±´ì„± ë¶€ì¡±)

---

## ğŸ’¡ ë¹ ë¥¸ ê°œì„ ì„ ìœ„í•œ ì¶”ì²œ ë°©ì•ˆ

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (1-2ì¼):

1. **Perturbation Wrapper êµ¬í˜„**
   - íŒŒì¼: `custom/wrappers/perturbation.py`
   - í•™ìŠµ ì¤‘ ëœë¤ ì™¸ë ¥ ì ìš©
   - ë¹„êµì  ì‰¬ìš´ êµ¬í˜„

2. **ë‹¤ì–‘í•œ mocap ì‚¬ìš©**
   ```yaml
   default_dataset_conf:
       task: [walk, run, pace]
   lafan1_dataset_conf:
       task: [walk1_subject1, run1_subject1]
   ```

3. **í•™ìŠµ ì‹œê°„ ì—°ì¥**
   - 75M â†’ 150M timesteps
   - ë” ë§ì€ ë°ì´í„°ë¡œ ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ

### ë‹¨ê¸° ëª©í‘œ (1-2ì£¼):

1. **ASE ë…¼ë¬¸ êµ¬í˜„ ì‹œë„**
   - GitHubì— ê³µê°œëœ êµ¬í˜„ ì°¸ê³ 
   - loco-mujocoì— ë§ê²Œ ìˆ˜ì •

2. **Domain Randomization ì¶”ê°€**
   - ë¬¼ë¦¬ íŒŒë¼ë¯¸í„° ëœë¤í™”
   - ë…¸ì´ì¦ˆ ì¶”ê°€

3. **Recovery Policy ë³„ë„ í•™ìŠµ**
   - Falling â†’ Recovery ì „ìš© policy
   - Main policyì™€ í†µí•©

### ì¥ê¸° ëª©í‘œ (1-2ê°œì›”):

1. **Hierarchical RL êµ¬í˜„**
   - High-level + Low-level ë¶„ë¦¬
   - Meta-learning ì ìš©

2. **ì‹¤ì œ ë¡œë´‡ í…ŒìŠ¤íŠ¸ ì¤€ë¹„**
   - Sim-to-real ê¸°ë²• ì ìš©
   - Safety layer êµ¬í˜„

---

## ğŸ”— ì°¸ê³  ìë£Œ

### í•µì‹¬ ë…¼ë¬¸:
1. **AMP** (í˜„ì¬ ì‚¬ìš©): [arXiv:2104.02180](https://arxiv.org/abs/2104.02180)
2. **ASE**: [arXiv:2205.01906](https://arxiv.org/abs/2205.01906)
3. **PHC**: [arXiv:2305.06456](https://arxiv.org/abs/2305.06456)
4. **DeepMimic**: [arXiv:1804.02717](https://arxiv.org/abs/1804.02717)

### êµ¬í˜„ ì½”ë“œ:
- AMP Official: https://github.com/xbpeng/DeepMimic
- ASE: https://github.com/nv-tlabs/ASE
- PHC: https://github.com/ZhengyiLuo/PHC

### LocoMuJoCo ê´€ë ¨:
- ê³µì‹ ë¬¸ì„œ: https://loco-mujoco.readthedocs.io/
- ì˜ˆì œ: `/home/tinman/loco-mujoco/examples/`
- 22,000+ mocap datasets í™œìš© ê°€ëŠ¥!

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

### ì‹¤í—˜ 1: Perturbation Training
```bash
# 1. Perturbation wrapper êµ¬í˜„
# 2. conf.yaml ìˆ˜ì •
# 3. 150M timesteps í•™ìŠµ
# 4. ë¹„êµ í‰ê°€
```

**ì˜ˆìƒ ê²°ê³¼:**
- ì™¸ë ¥ ëŒ€ì‘ ëŠ¥ë ¥ 3-5ë°° í–¥ìƒ
- Episode length ì¦ê°€ (ë” ì˜¤ë˜ ì„œìˆìŒ)
- Recovery rate í–¥ìƒ

### ì‹¤í—˜ 2: Multi-task Learning
```yaml
default_dataset_conf:
    task: [walk, run, pace, trot]
```

**ì˜ˆìƒ ê²°ê³¼:**
- ë‹¤ì–‘í•œ ì†ë„ ì œì–´ ê°€ëŠ¥
- ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜(transition)
- Generalization ëŠ¥ë ¥ í–¥ìƒ

### ì‹¤í—˜ 3: ASE êµ¬í˜„
```
1. ASE ì½”ë“œ ë¶„ì„
2. LocoMuJoCo í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
3. ëŒ€ê·œëª¨ mocap dataset í™œìš© (22,000+ clips)
4. ë¹„êµ ì‹¤í—˜
```

**ì˜ˆìƒ ê²°ê³¼:**
- State-of-the-art robustness
- ë§¤ìš° ë‹¤ì–‘í•œ ë™ì‘ ë ˆí¼í† ë¦¬
- ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ì¤€ ê²°ê³¼

---

**ê²°ë¡ :**
í˜„ì¬ ëª¨ë¸ì€ **êµìœ¡ìš©/ê¸°ì´ˆ ì—°êµ¬ìš©ìœ¼ë¡œ ì í•©**í•˜ì§€ë§Œ, ì‹¤ìš©ì„±ì„ ìœ„í•´ì„œëŠ” **perturbation trainingê³¼ ë” advancedí•œ ì•Œê³ ë¦¬ì¦˜**ì´ í•„ìš”í•©ë‹ˆë‹¤. ê°€ì¥ ë¹ ë¥¸ ê°œì„  ë°©ë²•ì€ **Perturbation Wrapperë¥¼ ì¶”ê°€í•˜ê³  í•™ìŠµ ì‹œê°„ì„ ëŠ˜ë¦¬ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.
